#!/bin/bash
# please check the namenode log Path previously, what's more, it requires the jobtracker's log is on the same node.
# it will analyze the split time of every "big" job.
# 1. Defining "big" job, whose jobid occurred most times in Namenode's log.Select the top 20 jobs
# 2. check jobID, in the NameNode, and caculate the intervel between beginning split" to "split ended"
# 3. sort split time and output into perday split_time_sorted_$suffix_date.
filename=$1
echo "find all jobID from:"$1;
suffix_date=`echo "$filename"| grep -oP "\d\d\d\d-\d\d-\d\d"`

echo "check clean "split_time_sorted_$suffix_date
rm -rf split_time_$suffix_date
rm -rf split_time_sorted_$suffix_date
for i in `cat $filename | grep -oP "job_\d+_\d+"| sort| uniq -c | sort -k1nr | head -n 20| grep -oP "job_\w+" `;
do
    splitSize=`grep "Input size for job "$i" ../hadoop/hadoop-hadoopmc-jobtracker-zw-hadoop-master.log.$suffix_date | cut -d "=" -f 2,3`
    grep $i $filename |awk '/setPermission.*job.jar/,/create.*job.split/'|grep -oP "\d\d:\d\d:\d\d"|awk -v jobid="$i" -v splitLog="$splitSize" -F":" 'NR>1{printf("%6d %s %s %s %s\n",$1*3600+$2*60+$3-n,pre,$0,jobid,splitLog);}{pre=$0;n=$1*3600+$2*60+$3}' >> split_time_$suffix_date ;
done

echo "write split time into :"split_time_sorted_$suffix_date cat split_time_$suffix_date| sort -k1nr >> split_time_sorted_$suffix_date
rm -rf split_time_$suffix_date
